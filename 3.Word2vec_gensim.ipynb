{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\"안녕 만나서 반갑다.\", \"만나서 반가워~\",\"너 이름이 뭐냐\",\"나는 널 좋아해\", \"나는 널 싫어하는데?\",\"파이토치랑 텐서플로우 뭐 쓰지\",\n",
    "                \"글쎄, 파이토치가 딥러닝 공부하기엔 짱인듯\", \"그렇지? 텐서플로우도 딥러닝 프레임워크이긴 하지만 어렵댔어\", \"응 맞아 파이토치가 짱이지\",\n",
    "               \"우선은 쉽고 직관적이잖아 파이토치가\", \"그건 진짜 장점이네\",\"아 근데 겁나 배고프다 뭐 먹지\",\"치킨 먹자\",\"치킨 말고 삼겹살 먹자\",\"아냐 치킨이 짱이야\",\n",
    "               \"그럼 치킨이랑 삼겹살 둘 다 먹자\",\"그래 그건 좋아\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = Kkma()\n",
    "tokenized = [tagger.morphs(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(tokenized, size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "# [[token,token,...],...,[token,token,....]]\n",
    "# size : 임베딩 시킬 차원\n",
    "# window : 단어 기준 몇 번째 단어까지 예측할 것인지 (디폴트가 skip-gram 구조)\n",
    "# min_count : 단어 포함시킬 때, 최소 빈도 기준 (n번 이상 등장한 단어만 취급한다)\n",
    "# workers : 코어 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('네', 0.7222025990486145),\n",
       " ('짱', 0.5669279098510742),\n",
       " ('알', 0.4998699426651001),\n",
       " ('듯', 0.42780801653862),\n",
       " ('고', 0.3891119956970215),\n",
       " ('우선', 0.3482539653778076),\n",
       " ('맞', 0.34092599153518677),\n",
       " ('직관적', 0.33713650703430176),\n",
       " ('이', 0.33406636118888855),\n",
       " ('는데', 0.32633858919143677)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"토치\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"word_vector_sample.bin\",binary=True) # 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 Pretrained word vector 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"word_vector_sample.bin\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02632598, -0.01812573,  0.03403983,  0.03408416, -0.03482538,\n",
       "        0.00932996,  0.00794745, -0.02320742,  0.02260346, -0.00835045],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vectors[\"토치\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
